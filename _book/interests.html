<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.253">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Knowledge - 2&nbsp; Interests</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./beliefs.html" rel="next">
<link href="./background.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Interests</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Knowledge: <br>A Human Interest Story</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle sidebar-tool" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./background.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Background</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./interests.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Interests</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./beliefs.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Belief</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./knowledge.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Knowledge</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./inquiry.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Inquiry</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ties.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Ties</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./changes.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Changes</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rationality.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Rationality</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./evidence.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Evidence</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./power.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Power</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar"><div class="quarto-margin-header"><div class="margin-header-item">
<p>&nbsp;</p>
</div></div>
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-redblue" id="toc-sec-redblue" class="nav-link active" data-scroll-target="#sec-redblue"><span class="toc-section-number">2.1</span>  Red or Blue?</a></li>
  <li><a href="#sec-fourfamilies" id="toc-sec-fourfamilies" class="nav-link" data-scroll-target="#sec-fourfamilies"><span class="toc-section-number">2.2</span>  Four Families</a></li>
  <li><a href="#sec-orthodox" id="toc-sec-orthodox" class="nav-link" data-scroll-target="#sec-orthodox"><span class="toc-section-number">2.3</span>  Against Orthodoxy</a>
  <ul class="collapse">
  <li><a href="#sec-orthodoxmoore" id="toc-sec-orthodoxmoore" class="nav-link" data-scroll-target="#sec-orthodoxmoore"><span class="toc-section-number">2.3.1</span>  Moore’s Paradox</a></li>
  <li><a href="#sec-superknow" id="toc-sec-superknow" class="nav-link" data-scroll-target="#sec-superknow"><span class="toc-section-number">2.3.2</span>  Super Knowledge to the Rescue?</a></li>
  <li><a href="#sec-probrescue" id="toc-sec-probrescue" class="nav-link" data-scroll-target="#sec-probrescue"><span class="toc-section-number">2.3.3</span>  Rational Credences to the Rescue?</a></li>
  <li><a href="#sec-orthodoxevidence" id="toc-sec-orthodoxevidence" class="nav-link" data-scroll-target="#sec-orthodoxevidence"><span class="toc-section-number">2.3.4</span>  Evidential Probability</a></li>
  </ul></li>
  <li><a href="#sec-oddsandstakes" id="toc-sec-oddsandstakes" class="nav-link" data-scroll-target="#sec-oddsandstakes"><span class="toc-section-number">2.4</span>  Odds and Stakes</a></li>
  <li><a href="#sec-whatinterests" id="toc-sec-whatinterests" class="nav-link" data-scroll-target="#sec-whatinterests"><span class="toc-section-number">2.5</span>  Theoretical Interests Matter</a></li>
  <li><a href="#sec-global" id="toc-sec-global" class="nav-link" data-scroll-target="#sec-global"><span class="toc-section-number">2.6</span>  Global Interest Relativity</a></li>
  <li><a href="#sec-neutrality" id="toc-sec-neutrality" class="nav-link" data-scroll-target="#sec-neutrality"><span class="toc-section-number">2.7</span>  Neutrality</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-interests" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Interests</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="sec-redblue" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="sec-redblue"><span class="header-section-number">2.1</span> Red or Blue?</h2>
<p>The key argument that knowledge is interest-relative starts with a puzzle about a game. Here are the rules of the game, which I’ll call the Red-Blue game.</p>
<ol type="1">
<li>Two sentences will be written on the board, one in red, one in blue.</li>
<li>The player will make two choices.</li>
<li>First, they will pick a colour, red or blue.</li>
<li>Second, they say whether the sentence in that colour is true or false.</li>
<li>If they are right, they win. If not, they lose.</li>
<li>If they win, they get $50, and if they lose, they get nothing.</li>
</ol>
<p>Our player is Anisa. She has been reading some medieval history, and last night was reading about the Battle of Agincourt. She was amused to see that it too place on her birthday, October 25, and in 1415, precisely 600 years before her own birthday. The book says all these things about the Battle of Agincourt because they are actually true, and when she read the book, Anisa believed them. She believed them because she had lots of independent evidence that the book was reliable (it came from a respected author and publisher, it didn’t contradict her well-grounded background beliefs), and she was sensitive to that evidence of its reliability. And, indeed, the book was generally reliable, as well as accurate on this point.</p>
<p>Anisa comes to know that she is playing the Red-Blue game, and that these are its rules. She does not come to know any other relevant fact about the game. When the game starts, the following two sentences are written on the board, the first in red, the second in blue.</p>
<ul>
<li>Two plus two equals four.</li>
<li>The Battle of Agincourt took place in 1415.</li>
</ul>
<p>Anisa looks at this, thinks to herself, “Oh, my book said that the Battle of Agincourt was in 1415, so (given the rules of the game) playing Blue-True will be as good as any other play, so I’m playing Blue-True. Playing Red-True would get the same amount, since obviously two plus two is four, but I’m going to play Blue-True instead”. And that’s what she does, and she wins the $50.</p>
<p>Intuitively, Anisa’s move here is irrational. It doesn’t cost her anything - she gets the $50. And it’s not that irrational as these things go - she costs herself $50 in the somewhat distant worlds where her reliable book gets this fact wrong. But it was still irrational. She took a needless risk, when there was a simple safe option on the table.</p>
<p>I’m going to argue, at some length, that the best explanation of why it is irrational for Anisa to play Blue-True is that knowledge is interest-relative. When she was at home reading the book and just thinking about medieval history, Anisa knew that the Battle of Agincourt took place in 1415. When she was playing the game, and thinking about trying to get win $50, Anisa does not know this. When she is moved into the game situation, she loses some knowledge she previously had.</p>
<p>This kind of interest-relativity is often taken to be a wild and radical development in the theory of knowledge. And it is certainly a reform. In putting it forward, I’m not making a new reform proposal. It is set out, and defended, in works by Jeremy Fantl and Matthew McGrath <span class="citation" data-cites="FantlMcGrath2002 FantlMcGrath2009">(<a href="references.html#ref-FantlMcGrath2002" role="doc-biblioref">2002</a>, <a href="references.html#ref-FantlMcGrath2009" role="doc-biblioref">2009</a>)</span>, John Hawthorne <span class="citation" data-cites="Hawthorne2004">(<a href="references.html#ref-Hawthorne2004" role="doc-biblioref">2004</a>)</span>, and Jason Stanley <span class="citation" data-cites="Stanley2005">(<a href="references.html#ref-Stanley2005" role="doc-biblioref">2005</a>)</span>. But relative to the epistemological status quo circa 1990, it is different. But then again, factors that were widely held to affect knowledge according to the status quo of either today or of 1990 would have seemed wild and radical relative to the epistemological status quo circa 1960. The factors that make a belief safe, or sensitive, or reliable, or undefeated, were well outside the realms of factors that mid 20th century epistemologists thought relevant to knowledge. There are many things that are irrelevant to how probable a belief is that are relevant to whether it is knowledge, as the epistemological literature of the late 20th Century makes clear. The proposal here is that interests are one more addition to this motley bunch.</p>
<p>In the recent literature, arguments for and against interest-relativity to date have not focussed on examples like Anisa’s, but on examples like Blaise that I’ll present shortly. But the example of Anisa has a handful of notable predecessors. It’s structure is similar to the examples of low-cost checking that Bradley Armour-Garb <span class="citation" data-cites="ArmourGarb2011">(<a href="references.html#ref-ArmourGarb2011" role="doc-biblioref">2011</a>)</span> discusses. (Though he draws contextualist conclusions from these examples, not interest-relative ones.) And it is similar to some of the cases of three-way choice that Charity Anderson and John Hawthorne deploy in arguing against interest-relativity <span class="citation" data-cites="AndersonHawthorne2019a AndersonHawthorne2019b">(<a href="references.html#ref-AndersonHawthorne2019a" role="doc-biblioref">2019a</a>, <a href="references.html#ref-AndersonHawthorne2019b" role="doc-biblioref">2019b</a>)</span>. But mostly people have focussed on cases like the following one.</p>
<p>Last night, Blaise was reading the same book that Anisa was reading. And he too was struck by the fact that the Battle of Agincourt took place on October 25, 1415. Today he is visited by a representative of the supernatural world, and offered the following bet. (Blaise knows these are the terms of the bet, and doesn’t know anything else relevant.) If he declines the bet, life will go on as normal. If he accepts, one of two things will happen.</p>
<ul>
<li>If it is true that the Battle of Agincourt took place in 1415, an infant somewhere will receive one second’s worth of pure joy, of the kind infants often get playing peek-a-boo.</li>
<li>If it is false that the Battle of Agincourt took place in 1415, all of humanity will be cast into The Bad Place for all of eternity.</li>
</ul>
<p>Blaise takes the bet. The Battle of Agincourt was in 1415, and he can’t bear the thought of a lovable baby missing that second of pure joy.</p>
<p>Again, there is an intuition that Blaise did something horribly wrong here. And this intuition is probably best explained by letting knowledge be interest-relative. But the argument that the interest-relativity of knowledge is the very best explanation of what’s going on is, in my view, somewhat weaker in Blaise’s case than in Anisa’s. It’s not that I don’t accept the interest-relative explanation of the case; I do accept it. It’s rather that in Blaise’s case there are alternative interest-invariant explanations are somewhat more plausible. So I’ll focus on Anisa, not Blaise.</p>
<p>This choice of focus occasionally means that this book is less connected to the existing literature than I would like. I occasionally infer what a philosopher would say about cases like Anisa’s from what they have said about cases like Blaise’s. And probably in some cases I’ll get those inferences wrong. But I want to set out the best argument for the interest-relativity of knowledge that I know, and that means going via the example of Anisa.</p>
<p>Though I am starting with an example, and with an intuition about it, I am not starting with an intuition about what is known in the example. I don’t have any clear intuitions about what Anisa knows or doesn’t know while playing the Red-Blue game. The intuition that matters here is that her choice of Blue-True is irrational. It’s going to be a matter of inference, not intuition, that Anisa lacks knowledge.</p>
<p>And that inference will largely be by process of elimination. In <a href="#sec-fourfamilies">section&nbsp;<span>2.2</span></a> I will set out four possible things we can say about Anisa, and argue that one of them must be true. (The argument won’t appeal to any principles more controversial than the Law of Excluded Middle.) But all four of them, including the interest-relative view I favour, have fairly counterintuitive consequences. So something counterintuitive is true around here. And this puts a limit on how we can argue. At least one instance of the argument <em>this is counterintuitive, so it is false</em> must fail. And that casts doubt over all such arguments. This is a point that critics of interest-relativity haven’t sufficiently acknowledged, but it also puts constraints on how one can defend interest-relativity.</p>
<p>When Anisa starts playing the Red-Blue game, her practical situation changes. So you might think I’ve gone wrong in stressing Anisa’s interests, not her practical situation. I’ve put the focus on interests for two reasons. One is that if Anisa is totally indifferent to money, then there is no rational requirement to play Red-True. We need to posit something about Anisa’s interests to even get the data point that the interest-relative theory explains. The second reason, which I’ll talk about more in <a href="#sec-whatinterests">section&nbsp;<span>2.5</span></a>, is that sometimes we can lose knowledge due to a change not in our practical situation, but our theoretical interests.</p>
<p>In the existing literature, views like mine are sometimes called versions of <strong>subject-sensitive invariantism</strong>, since they make knowledge relevant to the stakes and salient alternatives available to the subject. But this is a bad name; of course whether a knowledge ascription is true is sensitive to who the subject of the ascription is. I know what I had for breakfast and you (probably) don’t. What is distinctive is which features of the subject’s situation that the interest-relative theory says are relevant, and calling it the interest-relative theory of knowledge makes it clear that it is the subject’s interests. In the past, I’ve called it <strong>interest-relative invariantism</strong>. But, for reasons I’ll say more about in <a href="#sec-neutrality">section&nbsp;<span>2.7</span></a>, I’m not committed to <em>invariantism</em> in this book. So it’s just the interest-relative theory, or IRT.</p>
</section>
<section id="sec-fourfamilies" class="level2 page-columns page-full" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="sec-fourfamilies"><span class="header-section-number">2.2</span> Four Families</h2>
<p>A lot of philosophers have written about cases like Anisa’s and Blaise’s over the last couple of decades. Relatedly, there are a huge number of theories that have been defended concerning these cases. Rather than describe them all, I’m going to start with a taxonomy of them. The taxonomy has some tricky edge cases, and it isn’t always trivial to classify a philosopher from their statements about the cases. But I find it a helpful way to start thinking about the possible moves available.</p>
<p>Our first family of theories are the <strong>sceptical</strong> theories. They deny that Anisa ever knew that the Battle of Agincourt was in 1415. The particular kind of sceptic I have in mind says that if someone’s epistemic position is, all things considered, better with respect to <em>q</em> than with respect to <em>p</em>, that person doesn’t know that <em>p</em>. The core idea for this sceptic, which perhaps they draw from work by Peter Unger <span class="citation" data-cites="Unger1975">(<a href="references.html#ref-Unger1975" role="doc-biblioref">1975</a>)</span>, is that knowledge is a maximal epistemic state, so any non-maximal state is not knowledge. The sceptics say that for almost any belief, Anisa’s belief that two plus two is four will have higher epistemic standing than that belief, so that belief doesn’t amount to knowledge.</p>
<p>Our second family of theories are what I’ll call <strong>epistemicist</strong> theories. The epistemicists say that Anisa’s reasoning is perfectly sound, and perhaps Blaise’s is too. They both know when the Battle of Agincourt took place, so they both know that the choices they take are optimal, so they are rational in taking those choices. The intuitions to the contrary are, say the epistemicist, at best confused. There is something off about Anisa and Blaise, perhaps, but it isn’t that these particular decisions are irrational.</p>
<div class="page-columns page-full"><p>It’s not essential to epistemicism, but one natural form of epistemicism takes on board Maria Lasonen-Aarnio’s point that act-level and agent-level assessments might come apart.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> On this version of epistemicism, taking the bet reveals something bad about Blaise’s character, and arguably manifests a vice, but the act itself is rational. It’s that last claim, that the actions like Blaise’s are rational, that is distinctive of epistemicism.</p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;See Lasonen-Aarnio <span class="citation" data-cites="Lasonen-Aarnio2010b Lasonen-Aarnio2014">(<a href="references.html#ref-Lasonen-Aarnio2010b" role="doc-biblioref">2010</a>, <a href="references.html#ref-Lasonen-Aarnio2014" role="doc-biblioref">2014</a>)</span> for more details on her view. In <em>Normative Externalism</em>, I describe the difference between act-level and agent-level assessments as the difference between asking whether what Anisa does is rational, and whether Anisa’s action manifests wisdom <span class="citation" data-cites="Weatherson2019">(<a href="references.html#ref-Weatherson2019" role="doc-biblioref">Weatherson 2019, 124–25</a>)</span>. The best form of epistemicism, I’m suggesting, says that Anisa and Blaise are rational but unwise. This isn’t Lasonen-Aarnio’s terminology, but otherwise I’m just coopting her ideas.</p></li></div></div>
<p>The third family is the family of <strong>pragmatist</strong> theories, and this family includes the interest-relative theory that I’ll defend. The pragmatists say that yesterday Anisa knew when the Battle of Agincourt was, but now she doesn’t. The change in her practical situation, combined with her interest in getting more money, destroys her knowledge.</p>
<p>And the final family are what I’ll call, a little tendentiously, the <strong>orthodox</strong> theories. Orthodoxy says that Anisa knew when the Battle of Agincourt was last night, since her belief satisfied every plausible criterion for testimonial knowledge. And it says she knows it today, since changing practical scenarios or interests like this doesn’t affect knowledge. But it also says that the actions that Anisa and Blaise take are wrong; they are both irrational, and Blaise’s is immoral. And that is true because of how risky the actions are. So knowing that what you are doing is for the best is consistent with your action being faulted on epistemic grounds.</p>
<p>My reading of the literature is that a considerable majority of philosophers writing on these cases are orthodox. (Hence the name!) But I can’t be entirely sure, because a lot of these philosophers are more vocal about opposing pragmatist views than they are about supporting any particular view. There are some views that are clearly orthodox in the sense I’ve described, and I really think most of the people who have opposed pragmatist treatments of cases like Anisa’s and Blaise’s are orthodox, but it’s possible more of them are sceptical or epistemicist than I’ve appreciated.</p>
<p>Calling this last family orthodox lets me conveniently label the other three families as heterodox. And this lets me state what I hope to argue for in this book: the interest-relative treatment of these cases is correct; and if it isn’t, then at least some pragmatist treatment is correct; and if it isn’t, then at least some heterodox treatment is correct.</p>
<p>It’s worth laying out the interest-relative case in some detail, because we can only properly assess the options holistically. Every view is going to have some very counterintuitive consequences, and we can only weigh them up when we see them all laid out. The last claim, that every view has counterintuitive consequences, deserves some defence. I’ll say much more about the challenges orthodoxy faces in <a href="#sec-orthodox">section&nbsp;<span>2.3</span></a>. But just to set out a simple version of the problems for each theory, observe all of the following look true.</p>
<ul>
<li>Sceptical theories imply that when Anisa is reading her book, she doesn’t gain knowledge even though the book is reliable and she believes it because of a well-supported belief in its reliability.</li>
<li>Epistemicist theories imply that Anisa and Blaise make rational choices, even though they take what look like absurd risks.</li>
<li>Pragmatist theories say that offering someone a bet can cause them to lose knowledge and, presumably, that withdrawing that offer can cause them to get the knowledge back.</li>
<li>Orthodox theories say that it is irrational to do something that one knows will get the best result simply because it might get a bad result.</li>
</ul>
<p>Much of what follows in this book, like much of what’s in this literature, will fall into one of two categories. Either it will be an attempt to sharpen one of these implausible consequences, so the view with that consequence looks even worse than it does now. Or it will be an attempt to dull one of them, by coming up with a version of the view that doesn’t have quite as bad a consequence. Sometimes this latter task is sophistry in the bad sense; it’s an attempt to make the implausible consequence of the theory harder to say, and so less of an apparent flaw on that ground alone. But sometimes it is valuable drawing of distinctions. That is, it is scholasticism in the good sense. It turns out that the allegedly plausible claim is ambiguous. On one disambiguation we have really good reason to believe it is true, on another the theory in question violates it, but on no disambiguation do we get a violation of something really well-supported. I hope that they work I do here to defend the interest-relative theory is more scholastic than sophistic, but I’ll leave that for others to decide.</p>
<p>Still, if all of the theories are implausible in one way or another, shouldn’t we look for an alternative? Perhaps we should look, but we won’t find any. At least if we define the theories carefully enough, the truth is guaranteed to be among them. Let’s try placing theories by asking three yes/no questions.</p>
<ol type="1">
<li>Does the theory say that Anisa knew last night that the Battle of Agincourt was in 1415? If no, the theory is sceptical; if yes, go to question 2.</li>
<li>Does the theory say that Anisa is rational to play Blue-True? If yes, the theory is epistemicist; if no, go to question 3.</li>
<li>Does the theory say that Anisa still knows that the Battle of Agincourt was in 1415, at the time she chooses to play Blue-True? If no, the theory is pragmatist; if yes, the theory is orthodox.</li>
</ol>
<p>That’s it - those are your options. There are two two points of clarification that matter, but I don’t think they make a huge difference.</p>
<p>The first point of clarification is really a reminder that these are families of views. It might be that one member of the family is considerably less implausible than other members. Indeed, I’ve changed my mind a fair bit about what is the best kind of pragmatist theory since I first started writing on this topic. And there are a lot of possible orthodox theories. Finding out the best version of these kinds of theories, especially the last two kinds, is hard work, but it is worth doing. But I very much doubt it will lessen the implausibility of endorsing a view from that family; some of the implausibility flows directly from how one answers the three questions.</p>
<p>The second point of clarification is that what I’ve really done here is classify what the different theories say about Anisa’s case. They may say different things about other cases. A theory might take an epistemicist stand on Anisa’s case, but an orthodox one on Blaise’s case, for example. Or it might be orthodox about Anisa, but would be epistemicist if the blue sentence was something much more secure, such as that the Battle of Hastings was in 1066. If this taxonomy is going to be complete, it needs to say something about theories that treat different cases differently. So here is the more general taxonomy I will use.</p>
<p>The cases I’ll quantify over have the following structure. The hero is given strong evidence for some truth <em>p</em>, and they believe it on the basis of that evidence. There are no defeaters, the belief is caused by the truth of the proposition in the right way, and in general all the conditions for knowledge that people worried about in the traditional (i.e., late twentieth century) epistemological literature are met. Then they are offered a choice, where one of the options will have an optimal outcome if <em>p</em>, but will not maximise expected value unless the probability of <em>p</em> is absurdly close to 1. And while hero’s evidence is strong, it isn’t that strong. Despite this, hero takes the risky option, using the fact that <em>p</em> as a key part of their reasoning. Now consider the following three questions.</p>
<ol type="1">
<li>In cases with this form, does the theory say that when the hero first forms the belief that <em>p</em>, they know that <em>p</em>? If the answer is that this is <em>generally</em> the case, then restrict attention to those cases where they do know that <em>p</em>, and move to question 2. Otherwise, the theory is sceptical.</li>
<li>In the cases that remain, is hero rational in taking the option that is optimal if <em>p</em>, but requires very high probability to maximise expected returns? If the answer is yes in <em>every</em> case, the theory is epistemicist. Otherwise, restrict attention to cases where this choice is irrational, and move to question 3.</li>
<li>In <em>any</em> of the cases that remain, does the fact that hero was offered the choice destroy their knowledge that <em>p</em>? If yes, the theory is pragmatic. If no, the theory is orthodox.</li>
</ol>
<p>So I’m taking epistemicism to be a very strong theory - it says that knowledge always suffices for action that is optimal given what’s known, and that offers of bets never constitute a loss of knowledge. The epistemicist can allow that the offer of a bet may cause a person to ‘lose their nerve’, and hence their belief that <em>p</em>, and hence their knowledge that <em>p</em>. But if they remain confident in <em>p</em>, they retain knowledge that <em>p</em>.</p>
<p>Pragmatism is a very weak theory - it says sometimes the offer of a bet can constitute a loss of knowledge. The justification for defending such a weak theory is that so many philosophers are aghast at the idea that practical considerations like this could ever be relevant to knowledge. So even showing that the existential claim is true, that sometimes practical issues matter, would be a big deal.</p>
<p>Orthodoxy is a weak claim on one point, and a strong claim on another. It says there are some cases where knowledge does not suffice for action - though it might take these cases to be very rare. It is common in defences of orthodoxy to say that the cases are quite rare, and use this fact to explain away intuitions that threaten orthodoxy. But it says that pragmatic factors never matter - so it can be threatened by a single case like Anisa.</p>
</section>
<section id="sec-orthodox" class="level2 page-columns page-full" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="sec-orthodox"><span class="header-section-number">2.3</span> Against Orthodoxy</h2>
<p>The orthodox view of cases like Blaise’s is that offering him the bet does not change what he knows, but still he is irrational to take the bet. In this section, I’m going to run through a series of arguments against the orthodox view. The reason I am making so many arguments is not that I lack confidence in any one of them. Rather, it is because the orthodox view is so widespread that we need to appreciate how many strange consequences it has.</p>
<section id="sec-orthodoxmoore" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="sec-orthodoxmoore"><span class="header-section-number">2.3.1</span> Moore’s Paradox</h3>
<p>Start by thinking about what the orthodox view says a rational person in Blaise’s situation would do. Call this rational person Chamari. According to the orthodox view, offering someone a bit does not make them lose knowledge. So Chamari still knows when the Battle of Agincourt was fought. But Chamari is rational, so Chamari will clearly decline the bet. Think about how Chamari might respond when you ask her to justify declining the bet.</p>
<blockquote class="blockquote">
<p>You: When was the Battle of Agincourt?<br>
Chamari: October 25, 1415.<br>
You: If that’s true, what will happen if you accept the bet?<br>
Chamari: A child will get a moment of joy.<br>
You: Is that a good thing?<br>
Chamari: Yes.<br>
You: So why didn’t you take the bet?<br>
Chamari: Because it’s too risky.<br>
You: Why is it risky?<br>
Chamari: Because it might lose.<br>
You: You mean the Battle of Agincourt might not have been fought in 1415.<br>
Chamari: Yes.<br>
You: So the Battle of Agincourt was fought in 1415, but it might not have been fought then?<br>
Chamari: Yes, the Battle of Agincourt was fought in 1415, but it might not have been fought then, and that’s why I’m not taking the bet.</p>
</blockquote>
<p>Chamari has given the best possible answer at each point. But she has ended up assenting to a Moore-paradoxical sentence. In particular, she has assented to a sentence of the form <em>p, but it might be that not p</em>. And it is very widely held that sentences like this cannot be rationally assented to. Since Chamari was, by stipulation, the model for what the orthodox view thinks a rational person is, this shows that the orthodox view is false.</p>
<p>There are three ways out of this puzzle, and none of them seems particularly attractive.</p>
<p>One is to deny that there’s anything wrong with where Chamari ends up. Perhaps in this case the Moore-paradoxical claim is perfectly assertable. I have some sympathy for the general idea that philosophers over-state the badness of Moore-paradoxicality <span class="citation" data-cites="MaitraWeatherson2010">(<a href="references.html#ref-MaitraWeatherson2010" role="doc-biblioref">Maitra and Weatherson 2010</a>)</span>. But I have to say, in this instance it seems like a terrible way to end the conversation.</p>
<p>Another is to deny that the fact that Chamari knows something licences her in asserting it. I’ve assumed in the argument that if Chamari knows that <em>p</em>, she can say that <em>p</em>. But maybe that’s too strong an assumption. The conversation, says this reply, goes off the rails at the very first line. But on this way of thinking, it is hard to know what the point of knowledge is. If knowing something isn’t sufficiently good reason to assert it, it is hard to know what would be.</p>
<p>The orthodox theorist has a couple of choices here, neither of them good. One is to say that although knowledge is not interest-relative, the epistemic standards for assertion are interest-relative. Basically, Chamari meets the epistemic standard for saying that <em>p</em> only if Chamari knows that <em>p</em> according to the (false!) interest-relative theory. But at this point, given how plausible it is that knowledge is closely connected with testimony, it seems we would need an excellent reason to not simply identify knowledge with this epistemic standard. The other is to say that there is some interest-invariant standard for assertion. But versions of Blaise’s case show that this standard would have to be something like Cartesian certainty. So most everything we say, every single day, would be norm violating. Such a norm is not plausible.</p>
<p>So we get to the third way out, one that is only available to a subset of orthodox theorists. We can say that ‘knows’ is context-sensitive, that in Chamari’s context the sentence “I know when the Battle of Agincourt was fought” is actually false, and those two facts explain what goes wrong in the conversation with Chamari. Armour-Garb <span class="citation" data-cites="ArmourGarb2011">(<a href="references.html#ref-ArmourGarb2011" role="doc-biblioref">2011</a>)</span>, who points out how much trouble non-contextualist orthodox theorists get into with these Moore-paradoxical claims, suggests a contextualist resolution of the puzzles. And this is probably the least bad way to handle the case, but it’s worth noting just how odd it is.</p>
<p>It’s not immediately obvious how to get from contextualism to a resolution of the puzzle. Chamari doesn’t use the verb ‘to know’ or any of its cognates. She does use the modal ‘might’, and the contextualist will presumably want to say that it is context sensitive. But that doesn’t look like a helpful way to solve the problem, since her assertion that the Battle might have been on a different day seems like the good part of what she says. What’s problematic is the unqualified assertion about when the battle was, in the context of explaining her refusal to bet. And we need some way of connecting contextualism about epistemic verbs to a claim about the inappropriateness of this assertion.</p>
<p>The standard move by contextualists here is to simply deny that there is a tight connection between knowledge and assertion <span class="citation" data-cites="DeRose2002 Cohen2004">(<a href="references.html#ref-DeRose2002" role="doc-biblioref">DeRose 2002</a>; <a href="references.html#ref-Cohen2004" role="doc-biblioref">Cohen 2004</a>)</span>. (So this is really a sophisticated form of the response I just rejected.) What they say instead is that there is a kind of meta-linguistic standard for assertion. It is epistemically responsible to say that <em>p</em> iff it would be true to say <em>I know that p</em>. And since it would not be true for Chamari to say she knows when the Battle of Agincourt was fought, she can’t responsibly say when it was fought.</p>
<p>The most obvious reason to reject this line of reasoning is that it is implausible that meta-linguistic norms like this exist. Imagine we were conversing with Chamari about her reasons for declining the bet in Bengali rather than English, but at every line a contribution with the same content was made. Would the reason her first answer was inappropriate be that some English sentence would be false if uttered in her context, or that some Bengali sentence would be false? If it’s an English sentence, it’s very weird that English would have this normative force over conversations in Bengali. If it’s Bengali, then it’s odd that the standard for assertion changes from language to language.</p>
<p>If there were a human language that didn’t have a verb for knowledge, then that last point could be made with particular force. What would the contextualists say is the standard for assertion in such a language? But there is, quite surprisingly, no such language <span class="citation" data-cites="Nagel2014">(<a href="references.html#ref-Nagel2014" role="doc-biblioref">Nagel 2014</a>)</span>. It’s still a bit interesting to think about possible languages that do allow for assertions, but do not have a verb for knowledge. Just what the contextualists would say is the standard for assertion in such a language is a rather delicate matter.</p>
<p>But rather than thinking about these merely possible languages, let’s return to English, and end with a variant of the conversation with Chamari. Imagine that she hasn’t yet been offered any bet, and indeed that when the conversation starts, we’re just spending a pleasant few minutes idly chatting about medieval history.</p>
<blockquote class="blockquote">
<p>You: When was the Battle of Agincourt?<br>
Chamari: October 25, 1415.<br>
You: Oh that’s interesting. Because you know there’s this bet that someone offered my friend Blaise, and I bet I could get them to offer it to you. If you were to accept it, and the Battle of Agincourt was in 1415, then a small child would get a moment of joy.<br>
Chamari: That’s great, I should take that bet.<br>
You: Well, wait a second, I should tell you what happens if the Battle turns out to have been on any other date. [You explain what happens in some detail.]<br>
Chamari: That’s awful, I shouldn’t take the bet. The Battle might not have been in 1415, and it’s not worth the risk.<br>
You: So you won’t take the bet because it’s too risky?<br>
Chamari: That’s right, I won’t take it because it’s too risky.<br>
You: Why is it risky?<br>
Chamari: Because it might lose.<br>
You: You mean the Battle of Agincourt might not have been fought in 1415.<br>
Chamari: Yes.<br>
You: Hang on, you just say it was fought in 1415, on October 25 to be precise.<br>
Chamari: That’s true, I did say that.<br>
You: Were you wrong to have said it?<br>
Chamari: Probably not; it was probably right that I said it.<br>
You: You probably knew when the battle was, but you don’t now know it?<br>
Chamari: No, I definitely didn’t know when the battle was, but it was probably right to have said it was in 1415.</p>
</blockquote>
<p>And you can probably see all sorts of ways of making Chamari’s position sound terrible. The argument I’m giving here is based on an argument against contextualism that John MacFarlane <span class="citation" data-cites="MacFarlane2005-Knowledge">(<a href="references.html#ref-MacFarlane2005-Knowledge" role="doc-biblioref">2005</a>)</span> gives, noting that contextualists have a particular problem with retraction. And Chamari’s position does sound very bad here. But I don’t want to lean too much weight on how she sounds. Every position in this area ends up saying some strange things. The very idea that the epistemic standard for assertion could be meta-linguistic is even more implausible than the idea that we should end up where Chamari does.</p>
</section>
<section id="sec-superknow" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="sec-superknow"><span class="header-section-number">2.3.2</span> Super Knowledge to the Rescue?</h3>
<p>Let’s leave Blaise and Chamari for a little and return to Anisa. The orthodox view agrees that it is irrational for Anisa to play Blue-True. So it needs to explain why this is so. IRT offers a simple explanation. If she plays Red-True, she knows she will get $50; if she plays Blue-True, she does not know that - though she knows she will get at most $50. So Red-True is the weakly dominant option; she knows it won’t do worse than any other option, and there is no other option that she knows won’t do worse than any other option.</p>
<p>The orthodox theorist can’t offer this explanation. They think Anisa knows that Blue-True will get $50 as well. So what can they offer instead? There are two broad kinds of explanation that can try. First, they might offer a structurally similar explanation to the one IRT gives, but with some other epistemic notion at its centre. So while Anisa knows that Blue-True will get $50, she doesn’t <em>super-know</em> this, in some sense. Second, they can try to explain the asymmetry between Red-True and Blue-True in probablistic, rather than epistemic, terms. I’ll discuss the first option in this subsection, and the probabilistic notion in the next subsection.</p>
<p>What do I mean her by <em>super-knows</em>? I mean this term to be a placeholder for any kind of relation stronger than knowledge that could play the right kind of role in explaining why it is irrational for Anisa to play Blue-True. So super-knowledge might be iterated knowledge. Anisa super-knows something iff she knows that she knows that … she knows it. And she super-knows that two plus two is four, but not that the Battle of Agincourt was in 1415. Or super-knowledge might be (rational) certainty. Anisa is (rationally) certain that two plus two is four, but not that the Battle of Agincourt was in 1415. Or it might be some other similar relation. My objection to the super-knowledge response won’t be sensitive to the details of how we understand super-knowledge.</p>
<p>What is going to be important is that super-knowledge, whatever it is, looks like an epistemic relation. In particular, Anisa super-knows a conjunction (that she is considering) iff she super-knows each of the conjuncts. So we can’t equate super-knowledge with rational credence above a threshold, because rational credence above a threshold doesn’t satisfy this constraint. I’ll come back to credence based explanations of Anisa’s case in the next subsection.</p>
<p>The fact that Anisa doesn’t super-know when the Battle of Agincourt was can’t explain the asymmetry between Red-True and Blue-True. In particular, it can’t explain why Anisa rationally must choose Red-True. This is because she doesn’t super-know that playing Red-True will win the $50. And that’s because she doesn’t super-know the rules of the game. She has ordinary testimonial knowledge of the rules, just like she has ordinary testimonial knowledge about the Battle of Agincourt. In the description of the game, I stipulated that she didn’t know anything relevant about the game set up other than what I said there. So she isn’t absolutely certain of the rules, and she doesn’t even know that she knows them. Maybe you think that last is unrealistic, and it is important that the example is realistic. But even on a realistic treatment of the game, she won’t super-know the rules. If testimony from careful historians can’t generate super-knowledge, neither can testimony from game-show hosts.</p>
<p>In fact, her knowledge of the rules of the game, in the sense that matters, is probably weaker than her knowledge of history. It is not unknown for game shows to promise prizes, then fail to deliver them, either because of malice or incompetence. Knowledge of the game rules, in particular knowledge that she will actually get $50 if she selects a true sentence, requires some knowledge of the future. And that seems harder to obtain than knowledge of what happened in history. After all, she has to know that there won’t be an alien invasion, or a giant asteroid, or an incompetent or malicious game organiser.</p>
<p>So there is no way of understanding ‘super-knows’ such that 1 is true and 2 is false.</p>
<ol type="1">
<li>Anisa super-knows that if she plays Red-True, she’ll win $50.</li>
<li>Anisa does not super-know that if she plays Blue-True, she’ll win $50.</li>
</ol>
<p>And that’s the kind of contrast we need in order for a super-knowledge based explanation of why she should play Red-True to work.</p>
<p>The point I’m making here, that in thinking about these games we need to attend to the player’s epistemic attitude towards the game itself, is not original. Dorit Ganson <span class="citation" data-cites="Ganson2019">(<a href="references.html#ref-Ganson2019" role="doc-biblioref">2019</a>)</span> uses this point for a very similar purpose, and in turn quotes Robert Nozick <span class="citation" data-cites="Nozick1981">(<a href="references.html#ref-Nozick1981" role="doc-biblioref">1981</a>)</span> making a similar point. But I’ve belaboured it here because it is so easily overlooked. It is easy to take things that one is told about a situation, such as the rules of a game that are being played, as somehow fixed and inviolable. They aren’t the kind of thing that can be questioned. But in any realistic case, that will not be how things are - at least if one assumes that only what is super-known can be taken as fixed.</p>
<p>This is why I rest more weight on Anisa’s case than on Blaise’s. I can’t appeal to your judgment about what a realistic version of Blaise’s case would be like, because there are no realistic versions of cases like Blaise’s. But Anisa’s case is very easy to imagine and understand. And we can ask what a realistic version of it would be like. And that version would be such that the player would know what the rules of the game are, but would also know that sometimes game shows don’t keep their promises, sometimes they don’t describe their own games accurately, sometimes players misinterpret or misunderstand instructions, and so on. This shouldn’t lead us to scepticism: Anisa knows what game she’s playing. But she doesn’t super-know what game she’s playing. And that means she doesn’t super-know that she’ll win if she plays Red-True.</p>
</section>
<section id="sec-probrescue" class="level3 page-columns page-full" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="sec-probrescue"><span class="header-section-number">2.3.3</span> Rational Credences to the Rescue?</h3>
<div class="page-columns page-full"><p>So imagine the orthodox theorist drops super-knowledge, and looks somewhere else. A natural alternative is to use credences. Assume that the probability that the rules of the game are as described is independent of the probabilities of the red and blue sentence. And assume that Anisa must, if she is to be rational, maximise expected utility. Then we get the natural result that Anisa should pick the sentence that is more probably true.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> And that can explain why she must choose Red-True, which is what the orthodox theorist needed to explain.</p><div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;Strictly speaking, we need one more assumption - namely that for any unexpected way for the game to be, the probability of it being that way is independent of the truth of both the red and blue sentences. But it’s natural to assume that this is true.</p></li></div></div>
<p>This kind of approach doesn’t really have any place for knowledge in its theory of action. One should simply maximise expected utility; since doing what one knows to be best might not maximise expected utility, we shouldn’t think knowledge has any particularly special role.</p>
<p>But there are many problems with this kind of approach. Several of these problems will be discussed elsewhere in this book at more length. I will point to where those problems are discussed rather than duplicate the discussion here. But some other problems I’ll address straight away.</p>
<p>Like the view discussed in <a href="#sec-orthodoxmoore">subsection&nbsp;<span>2.3.1</span></a> that separates knowledge from assertion, separating knowledge from action leads to strange consequences. As Timothy Williamson <span class="citation" data-cites="Williamson2005">(<a href="references.html#ref-Williamson2005" role="doc-biblioref">2005</a>)</span> points out, once we break apart knowledge from action in this way, it becomes hard to see the point of knowledge. And it’s worth pausing a bit more over the bizarreness of the claim that Blaise knows that taking the bet will work out for the best, but he shouldn’t take it - because of its possible consequences.</p>
<p>If one excludes knowledge from having an important role in one’s theory of decision, one ends up having a hard time explaining how dominance reasoning works. But it is a compulsory task for a theory of decision to explain how dominance reasoning works. Among other things, we need a good account of how dominance reasoning works in order to handle Newcomb problems, and we need to handle Newcomb problems in order to motivate and even state a careful theory of expected utility maximisation. That little argument was very compressed, but I’ll return frequently through the book to issues about dominance reasoning; for now I think it’s enough to leave it with this sketch.</p>
<p>Probabilistic models of reasoning and decision have their limits. But what we need to explain about the Red-Blue game goes beyond those limits. So probabilistic models can’t be the full story about the Red-Blue game. To see this, imagine for a second that the Blue sentence is not about the Battle of Agincourt, but is instead a slightly more complicated arithmetic truth, like <em>Thirteen times seventeen equals two hundred and twenty one</em>, or a slightly complicated logical truth, like ¬<em>q</em> → ((<em>p</em> → <em>q</em>) → ¬<em>p</em>). If either of those are the blue sentence, then it is still uniquely rational to play Red-True. But the probability of each of those sentences is one. So rational choice is more demanding than expected utility maximisation. In sections <a href="rationality.html#sec-lockecoin"><span>8.2</span></a> and <a href="rationality.html#sec-lockegames"><span>8.3</span></a> I’ll go over more cases of propositions whose probability is 1, but which should be treated as uncertain even it is certain that two plus two is four. The lesson is that we can’t just use expected utility maximisation to explain the Red-Blue game.</p>
<p>Finally, we need to understand the notion of probability that’s being appealed to in this explanation. It can’t be some purely subjective notion, like credence, because that couldn’t explain why some decisions are rational and others aren’t. If Anisa was subjectively certain that the Battle of Agincourt was in 1415, she would still be irrational to play Blue-True. And it can’t be some purely physical notion, like chance or frequency, because that won’t even get the cases right. (What is the chance, or frequency, of the Battle of Agincourt being in 1415?) It needs to be something like evidential probability. And that will run into problems in versions of the Red-Blue game where the Blue sentence is arguably (but not certainly) part of the player’s evidence. I’ll end my discussion of orthodoxy with a discussion of cases like these.</p>
</section>
<section id="sec-orthodoxevidence" class="level3 page-columns page-full" data-number="2.3.4">
<h3 data-number="2.3.4" class="anchored" data-anchor-id="sec-orthodoxevidence"><span class="header-section-number">2.3.4</span> Evidential Probability</h3>
<div class="page-columns page-full"><p>No matter which of these explanations the orthodox theorist goes for, they need a notion of evidence to support them.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> Let’s assume that we can find some doxastic attitude D such that Anisa can’t rationally stand in D to <em>Play Blue-True</em>, and that this is why she can’t rationally play Blue-True. Then we need to ask the further question, why doesn’t she stand in relation D to <em>Play Blue-True</em>? And presumably the answer will be that she lacks sufficient evidence. If she had optimal evidence about when the Battle of Agincourt was, she could play Blue-True, after all.</p><div class="no-row-height column-margin column-container"><li id="fn3"><p><sup>3</sup>&nbsp;This subsection is based on §2 of my <span class="citation" data-cites="Weatherson2018-WEAIEA-2">(<a href="references.html#ref-Weatherson2018-WEAIEA-2" role="doc-biblioref">2018</a>)</span>.</p></li></div></div>
<p>The orthodox theorist also has to have an interest-invariant account of evidence. I guess it’s logically possible to have evidence be interest-relative, but knowledge be interest-neutral, but it is very hard to see how one would motivate such a position.</p>
<p>And now we run into a problem. Imagine a version of the Red-Blue game where the blue sentence is something that, if known, is part of the player’s evidence. If it is still irrational to play Blue-True, then any orthodox explanation that relies on evidence sensitive notions (like super-knowledge or evidential probability) will be in trouble. The aim of this subsection is to spell out why this is.</p>
<p>So let’s imagine a new player for the red-blue game. Call her Parveen. She is playing the game in a restaurant. It is near her apartment in Ann Arbor, Michigan. Just before the game starts, she notices an old friend, Rahul, across the room. Rahul is someone she knows well, and can ordinarily recognise, but she had no idea he was in town. She thought Rahul was living in Italy. Still, we would ordinarily say that she now knows Rahul is in town; indeed that he is in the restaurant. As evidence for this, note that it would be perfectly acceptable for her to say to someone else, “I saw Rahul here”. Now the game starts.</p>
<ul>
<li>The red sentence is <em>Two plus two equals four</em>.</li>
<li>The blue sentence is <em>Rahul is in this restaurant</em>.</li>
</ul>
<p>And here is the problem. On the one hand, there is only one rational play for Parveen: Red-True. She hasn’t seen Rahul in ages, and she thought he was in Italy. A glimpse of him across a crowded restaurant isn’t enough for her to think that <em>Rahul is in this restaurant</em> is as likely as <em>Two plus two equals four</em>. She might be wrong about Rahul, so she should take the sure money and play Red-True. So playing the red-blue game with these sentences makes it the case that Parveen doesn’t know where Rahul is. This is another case where knowledge is interest-relative, and at first glance it doesn’t look very different to the other cases we’ve seen.</p>
<p>But take a second look at the story for why Parveen doesn’t know where Rahul is. It can’t be just that her evidence makes it certain that two plus two equals four, but not certain that Rahul is in the restaurant. At least, it can’t be that unless it is not part of her evidence that Rahul is in the restaurant. And if evidence is not interest-relative, then it is part of Parveen’s evidence that Rahul is in the restaurant. This isn’t something she infers; it is a fact about the world she simply appreciated. Ordinarily, it is a starting point for her later deliberations, such as when she deliberates about whether to walk over to another part of the restaurant to say hi to Rahul. That is, ordinarily it is part of her evidence.</p>
<p>So the orthodox theorist has a challenge. If they say that it is part of Parveen’s evidence that Rahul is in the restaurant, then they can’t turn around and say that the evidential probability that he is in the restaurant is insufficiently high for her to play Blue-True. After all, its evidential probability is one. If they say that it is no part of Parveen’s evidence that Rahul is in the restaurant because she is playing this version of the Red-Blue game, they give up orthodoxy. So they have to say that our evidence never includes things like Rahul is in the restaurant.</p>
<p>This can be generalised. Take any proposition such that if the red sentence was that two plus two is four and that proposition was the content of the blue sentence, then it would be irrational to play Blue-True. Any orthodox explanation of the Red-Blue game entails that this proposition is no part of your evidence - whether you are playing the game or not. But once we strip all these propositions out of your evidence, you don’t have enough evidence to rationally believe, or even rationally make probable, very much at all.</p>
<p>Descartes, via very different means, walked into a version of this problem. And his answer was to (implicitly) take us to be infallible observers of our own minds, and (explicitly) offer a theistic explanation for how we can know about the external world given just this psychologistic evidence. Nowadays, most people think that’s wrong on both counts: we can be rationally uncertain about even our own minds, and there is no good path from purely psychological evidence to knowledge of the external world. The orthodox theorist ends up in a state worse than Cartesian scepticism.</p>
</section>
</section>
<section id="sec-oddsandstakes" class="level2 page-columns page-full" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="sec-oddsandstakes"><span class="header-section-number">2.4</span> Odds and Stakes</h2>
<div class="page-columns page-full"><p>If orthodox views are wrong, then it is important to get clear on which heterodox view is most plausible.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> I’m defending a version of the pragmatic view. But it’s a different version to the most prominent versions defended in the literature. The difference can be most readily seen by looking at the class of cases that have motivated pragmatic views.</p><div class="no-row-height column-margin column-container"><li id="fn4"><p><sup>4</sup>&nbsp;This section is based on §3 of my <span class="citation" data-cites="Weatherson2016">(<a href="references.html#ref-Weatherson2016" role="doc-biblioref">2016</a>)</span>.</p></li></div></div>
<p>The cases involve a subject making a practical decision. The subject has a safe choice, which has a guaranteed return of <em>S</em>. And they have a risky choice. If things go well, the return of the risky choice is <em>S</em>&nbsp;+&nbsp;<em>G</em>, so they will gain <em>G</em> from taking the risk. If things go badly, the return of the risky choice is <em>S</em>&nbsp;‑&nbsp;<em>L</em>, so they will lose <em>L</em> from taking the risk. What it takes for things to go well is that a particular proposition <em>p</em> is true. All of this is known by the subject facing the choice. It’s also true (but not uncontroversially known by the subject) that they satisfy all the conditions for knowing <em>p</em> that would have been endorsed by a well-informed epistemologist circa 1997. (That is, by a proponent of the traditional view.) So <em>p</em> is true, and things won’t go badly for them if they take the risk. But still, in a lot of these cases, there is a strong intuition that they should not take the bet, and as I’ve just been arguing, that is hard to square with the idea that they know that <em>p</em>. So assuming the traditional view is right about the subject as they were before facing the practical choice, having this choice in front of them causes them to lose knowledge that <em>p</em>.</p>
<p>But what is it about these choices that triggers a loss of knowledge? There is a familiar answer to this, one explicitly endorsed by Hawthorne <span class="citation" data-cites="Hawthorne2004">(<a href="references.html#ref-Hawthorne2004" role="doc-biblioref">2004</a>)</span> and Stanley <span class="citation" data-cites="Stanley2005">(<a href="references.html#ref-Stanley2005" role="doc-biblioref">2005</a>)</span>. It is that they are facing a ‘high stakes’ choice. Now what it is for a choice to be high stakes is never made entirely clear, and Anderson and Hawthorne <span class="citation" data-cites="AndersonHawthorne2019a">(<a href="references.html#ref-AndersonHawthorne2019a" role="doc-biblioref">2019a</a>)</span> show that it is hard to provide an adequate definition in full generality. But in the simple cases described in the previous paragraph, it is easy enough to say what a high stakes case is. It just means that <em>L</em> is large. So one gets the suggestion that practical factors kick in when faced with a case where there is a chance of a large loss.</p>
<div class="page-columns page-full"><p>This is not the version of IRT defended in this book. Instead, the version of IRT defended here says that <em>L</em> matters, but only indirectly. What is (typically) true in these cases is that the subject should maximise expected utility relative to what they know.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> And taking the risky choice maximises expected utility only if this equation is true.</p><div class="no-row-height column-margin column-container"><li id="fn5"><p><sup>5</sup>&nbsp;This simplifies the relationship between rational choice and expected utility maximisation. Later in the book I’ll have to be much more careful about this relationship. See chapter <a href="ties.html"><span>6</span></a> for many more details.</p></li></div></div>
<p><span class="math display">\[
\frac{\Pr(\textit{p})}{1&nbsp;‑&nbsp;\Pr(\textit{p})}&nbsp;&gt;&nbsp;\frac{\textit{L}}{\textit{G}}
\]</span></p>
<p>The left hand side expresses the odds that <em>p</em> is true. The right hand side expresses how high those odds have to be before the risk is worth taking. If the equation fails to hold, then the risk is not worth taking. And if risk is not worth taking, then the subject doesn’t know that <em>p</em>.</p>
<p>Since the numerator of the right hand side is <em>L</em>, then one way to destroy knowledge that <em>p</em> is to present the subject with a situation where <em>L</em> is very high. But it isn’t the only way. Since the denominator of the right hand side is <span class="math inline">\(G\)</span>, another way to destroy knowledge that <em>p</em> is to present the subject with a situation where <span class="math inline">\(G\)</span> is very low.</p>
<p>In effect, we’ve seen such a situation with Anisa. But to make the parallel to Anisa’s case even clearer, consider Darja’s case. Darja has been reading books about World War One, and yesterday read that Franz Ferdinand was assassinated on St Vitus’s Day, June 28, 1914. She is now offered a chance to play a slightly unusual quiz game. She has to answer the question <em>What was the date of Franz Ferdinand’s assassination?</em> If she gets it right, she wins $50. If she gets it wrong, she wins nothing. Here’s what is strange about the game. She is allowed to Google the answer before answering. So here are the two live options for Darja. In the table, and in what follows, <em>p</em> is the proposition that Franz Ferdinand was indeed assassinated on June 28, 1914.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: right;">&nbsp;</th>
<th style="text-align: center;"><em>p</em></th>
<th style="text-align: center;">¬ <em>p</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">Say “June 28, 1914”</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: right;">Google the answer</td>
<td style="text-align: center;">$50&nbsp;‑&nbsp;ε</td>
<td style="text-align: center;">$50&nbsp;‑&nbsp;ε</td>
</tr>
</tbody>
</table>
<p>If Darja has her phone near her, and has cheap easy access to Google, then ε might be really low. In that case she should take the safe option; it’s the one that maximises expected utility. And that means she doesn’t know that <em>p</em>, even if she remembers reading it in a book that is actually reliable. Facing a long odds bet can cause knowledge loss, even in low stakes situations.</p>
<p>So I’m committed to the view that Darja loses knowledge in her relatively low stakes situation, and indeed I think that’s true. But I don’t think that because I have any kind of intuition that this is the correct thing to say about her. I don’t have any clear intuition about this case, and I’m certainly not taking any intuition about the case as a premise. What I am taking as a premise is that Darja should Google the answer in cases like this one; doing otherwise is taking a bad risk. And the best explanation of why this is a bad risk is that she doesn’t know when Franz Ferdinand was assassinated. So practical interests can matter even in relatively low stakes cases.</p>
<p>I’m not the first to focus on these long odds/low stakes cases. Jessica Brown <span class="citation" data-cites="Brown2008">(<a href="references.html#ref-Brown2008" role="doc-biblioref">2008, 176</a>)</span> notes that these cases raise problems for the stakes-centric version of IRT. And Anderson and Hawthorne <span class="citation" data-cites="AndersonHawthorne2019a">(<a href="references.html#ref-AndersonHawthorne2019a" role="doc-biblioref">2019a</a>)</span> argue that once we get beyond the simple two-state/two-option choices, it isn’t at all easy to say what situations are and are not high-stakes choices. These cases are not problems for the version of IRT that I defend, since this version gives no role to stakes.</p>
</section>
<section id="sec-whatinterests" class="level2 page-columns page-full" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="sec-whatinterests"><span class="header-section-number">2.5</span> Theoretical Interests Matter</h2>
<div class="page-columns page-full"><p>When saying why I called my theory IRT, one of the reasons I gave was that I wanted theoretical, and not just practical, interests to matter to knowledge.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> This is also something of a break with the existing literature. After all, Jason Stanley’s book on interest-relative epistemology is called <em>Knowledge and Practical Interests</em>. And he defends a theory on which what an agent knows depends on the practical questions they face. But there are strong reasons to think that theoretical reasons matter as well.</p><div class="no-row-height column-margin column-container"><li id="fn6"><p><sup>6</sup>&nbsp;This section is based on §4 of my <span class="citation" data-cites="Weatherson2017-WEAII">(<a href="references.html#ref-Weatherson2017-WEAII" role="doc-biblioref">2017</a>)</span>.</p></li></div></div>
<p>In <a href="#sec-oddsandstakes">section&nbsp;<span>2.4</span></a>, I suggested that someone knows that <em>p</em> only if the rational choice to make would also be rational given <em>p</em>. That is, someone knows that <em>p</em> only if the answer to the question <em>What should I do?</em> is the same unconditionally as it is conditional on <em>p</em>. My preferred version of IRT generalises this approach. Someone knows that <em>p</em> only if the rational answer to a question she is interested in is the same unconditionally as it is conditional on <em>p</em>. Interests matter because they determine just what it is for the person to be interested in a question. Are the questions, in this sense, always practical questions, or do they also include theoretical questions? There are two primary motivations for allowing theoretical interests as well as practical interests to matter.</p>
<p>The first comes from what Jeremy Fantl and Matthew McGrath call the Unity Thesis&nbsp;<span class="citation" data-cites="FantlMcGrath2009">(<a href="references.html#ref-FantlMcGrath2009" role="doc-biblioref">Fantl and McGrath 2009, 73–76</a>)</span>. They argue that whether or not <em>p</em> is a reason for someone is independent of whether they are engaged in practical or theoretical deliberation. And the intuition supporting this is quite clear. Consider two people with the same background thinking about the question <em>What to do in situation S</em>. One of them is in <em>S</em>, the other is just thinking about it as an idle fantasy. Any reasoning one can properly do, the other can properly do. But one is facing a theoretical question, the other a practical question. So the difference between theoretical and practical questions can’t be relevant.</p>
<p>Let’s make that a little less abstract. Imagine Anisa is not actually faced with the choice between Red-True, Blue-True, Red False and Blue-False with these particular red and blue sentences. In fact, she has no practical decision to make that turns on the date of the Battle of Agincourt. But she is idly musing over what she would do if she were playing that game. (Perhaps because she is reading this book.) If she knows when the battle was, then she should be indifferent between Red-True and Blue-True. After all, she knows they will both win $50. But intuitively she should think Red-True is preferable, even in the abstract setting. And this seems to be the totally general case.</p>
<p>The general lesson is that if whether one can take <em>p</em> for granted is relevant to the choice between A and B, it is similarly relevant to the theoretical question of whether one would choose A or B, given a choice. And since those questions should receive the same answer, if <em>p</em> can’t be known while making the practical deliberation between A and B, it can’t be known while musing on whether A or B is more choiceworthy.</p>
<p>There is a second reason for including theoretical interests in what’s relevant to knowledge. There is something odd about reasoning from the probability of <em>p</em> is precisely <em>x</em>, to <em>p</em>, in any case where <em>x</em>&nbsp;&lt;&nbsp;1. It is a little hard to say, though, why this is problematic. We often take ourselves to know things on grounds that we would admit, if pushed, are probabilistic. The version of IRT that includes theoretical interests explains this oddity. If we are consciously thinking about whether the probability of <em>p</em> is <em>x</em>, then that’s a relevant question to us. Conditional on <em>p</em>, the answer to that question is clearly no, since conditional on <em>p</em>, the probability of <em>p</em> is 1. So anyone who is thinking about the precise probability of <em>p</em>, and not thinking it is 1, is not in a position to know <em>p</em>. And that’s why it is wrong, when thinking about <em>p</em>’s probability, to infer <em>p</em> from its high probability.</p>
<p>Putting the ideas so far together, we get the following picture of how interests matter. Someone knows that <em>p</em> only if the evidential probability of <em>p</em> is close enough to certainty for all the purposes that are relevant, given their theoretical and practical interests. Assuming the background theory of knowledge is non-sceptical, this will entail that interests matter.</p>
</section>
<section id="sec-global" class="level2 page-columns page-full" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="sec-global"><span class="header-section-number">2.6</span> Global Interest Relativity</h2>
<p>IRT was introduced as a thesis about knowledge. I’m going to argue in <a href="rationality.html">chapter&nbsp;<span>8</span></a> that it also extends to rational belief. Not every case where interests matter to knowledge generates a Dharmottara case. But we need not stop there. At the extreme, we could argue that every epistemologially interesting notion is interest-relative. Doing so gives us a global version of IRT. And that is what I’m going to defend here.</p>
<p>Jason Stanley <span class="citation" data-cites="Stanley2005">(<a href="references.html#ref-Stanley2005" role="doc-biblioref">2005</a>)</span> comes close to defending a global version. He notes that if one has both IRT, and a ‘knowledge first’ epistemology &nbsp;<span class="citation" data-cites="Williamson2000">(<a href="references.html#ref-Williamson2000" role="doc-biblioref">Williamson 2000</a>)</span>, then one is a long way to towards global IRT. Even if one doesn’t accept the whole knowledge first package, but just accepts the thesis that evidence is all and only what one knows, then one is a long way towards globalism. After all, if evidence is interest-relative, then probability, justification, rationality, and evidential support are interest-relative too.</p>
<p>That’s close to the path I’ll take to global IRT, but not exactly it. In <a href="evidence.html">chapter&nbsp;<span>9</span></a> I’m going to argue that evidence is indeed interest-relative, and so all those other notions are interest-relative too. But the version of IRT I defend implies that evidence is a subset of knowledge.</p>
<div class="page-columns page-full"><p>There is a deep puzzle here for IRT. On the one hand, the arguments for IRT look like they will generalise to arguments for the interest-relativity of evidence.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> On the other hand, the simplest explanation of cases like Anisa’s presupposes that we can identify Anisa’s evidence independent of her interests. That simple explanation says that Anisa shouldn’t play Blue-True because the evidential probability of the blue sentence being true is lower than the evidential probability of the red sentence being true. And since she can’t rationally play Blue-True, it follows that she mustn’t know that the blue sentence is true. If evidence is identified independently, this looks like it might generalise into a nice story about when changes of interests lead to changes of knowledge. But the story looks much less nice if evidence is also interest-relative, and it is.</p><div class="no-row-height column-margin column-container"><li id="fn7"><p><sup>7</sup>&nbsp;I was first convinced of this by conversations with Tom Donaldson some years back.</p></li></div></div>
<p>The aim of <a href="evidence.html">chapter&nbsp;<span>9</span></a> is to tell a story that avoids the worst of these problems. On the story I’ll tell, evidence is indeed interest-relative. And that means we can’t tell a simple story about precisely when changes in interests will lead to changes in knowledge. But it will still be true that people lose knowledge when the evidential probability of a proposition is no longer high enough for them to take it for granted with respect to every question they are interested in. And I will be able to say how interests impact evidence in a way that doesn’t require antecedently identifying how interests impact knowledge, so the story will still be somewhat reductive. But it won’t be as simple a story as one might hope to tell, or indeed that I tried to tell in earlier work.</p>
</section>
<section id="sec-neutrality" class="level2 page-columns page-full" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="sec-neutrality"><span class="header-section-number">2.7</span> Neutrality</h2>
<p>This book defends, at some length, the idea that knowledge is interest-relative. But I’m staying neutral on a number of other topics in the vicinity.</p>
<div class="page-columns page-full"><p>Most notably, I’m not taking any stand on whether contextualist theories of knowledge are true or false. If you think that contextualism is true, then what I’m defending is that the view that ‘knowledge’ picks out in this context, and in most other contexts, is interest-relative.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn8"><p><sup>8</sup>&nbsp;I am opposed to something that many contextualists appeal to in defending and motivating their theory, namely meta-linguistic norms on assertion, reasoning, and action. But that opposition doesn’t extend to contextualism in general.</p></li></div></div>
<p>Contextualist theories of knowledge have a lot in common with interest-relative theories. The kind of cases that motivate the interest-relative theories, cases like Anisa’s and Blaise’s, also motivate contextualism. They might even be seen as competitors, since they are offering rival explanations of similar phenomena. But they are not strictly inconsistent. Consider principles A and B below.</p>
<ol type="A">
<li>A’s utterance that <em>B knows that p</em> is true only if for any question <em>Q?</em> in which A is interested, the rational answer for B to give is the same unconditionally as it is conditional on <em>p</em>.</li>
<li>A’s utterance that <em>B knows that p</em> is true only if for any question <em>Q?</em> in which B is interested, the rational answer for B to give is the same unconditionally as it is conditional on <em>p</em>.</li>
</ol>
<div class="page-columns page-full"><p>I endorse principle B, and that’s why I endorse an interest-relative theory of knowledge. If I endorsed principle A, then I would be (more or less) committed to a contextualist theory of knowledge. And principle A is not inconsistent with principle B.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn9"><p><sup>9</sup>&nbsp;There is a technical difficulty in how to understand one person answering an infinitival question that another person is asking themselves. But the points I’m making in this section aren’t sensitive to this level of technical detail.</p></li></div></div>
<p>It isn’t hard to see why cases like Anisa and Blaise can move one to endorse principle A, and hence contextualism. It would be very odd for Anisa to say “This morning, I knew the Battle of Agincourt was in 1415.” That’s odd because she can’t now take it as given that the Battle of Agincourt was in 1415, and in some sense she wasn’t in any better or worse evidential position this morning with respect to the date of the battle. Perhaps, and this is the key point, it would even be false for Anisa to say this now. The contextualist, especially the contextualist who endorses principle A, has a good explanation for why that’s false. The interest-relative theorist doesn’t have anything to say about that. Personally I think it’s not obvious whether this would be false for Anisa to say, or merely inappropriate, and even if it is false, there may be decent explanations of this that are not contextualist. (For instance, maybe knowledge is sensitive to what one will know. Or maybe some kind of relativist theory is true.) But there is clearly an argument for contextualism here. And it isn’t one that I’m going to endorse or reject.</p>
<p>As I’ve already noted, I’m making heavy use of the principle that Jessica Brown calls K-Suff. I’m going to defend that at much greater length in what follows. What I’m not defending is the converse of that principle, what she calls K-Nec.</p>
<dl>
<dt>K-Nec</dt>
<dd>
<p>An agent can properly use <em>p</em> as a reason for action only if she knows that <em>p</em>.</p>
</dd>
</dl>
<p>The existing arguments for and against K-Nec are intricate and interesting, and I don’t have anything useful to add to them. All I will note is that the argument of this chapter doesn’t rely on K-Nec, and I’m mostly going to set it aside. I do think the motivations described in the previous chapter are stronger if one has K-Nec in place. But the formal argument doesn’t rely on it.</p>
<p>And I’m obviously not going to offer anything like a full theory of knowledge. I am defending a particular necessary condition on knowledge. That condition entails that knowledge is interest-relative given some common-sense assumptions about how widespread knowledge is. And that’s just about as far as I’ll go.</p>
<p>I will be making one claim about how interests typically enter into the theory of knowledge. I’ll argue that there is a certain kind of defeater. A person only knows that <em>p</em> if the belief that <em>p</em> coheres in the right way with the rest of their attitudes. What’s ‘the right way’? That, I argue, is interest-relative. In particular, some kinds of incoherence are compatible with knowledge if the incoherence concerns questions that are not interesting.</p>
<p>So the impact of interests is (typically) very indirect. Even if the other conditions for knowledge are satisfied, someone might fail to know something because it doesn’t cohere well with the rest of their beliefs. But there is an exception to this exception clause. Incoherence with respect to uninteresting questions is compatible with knowledge.</p>
<p>This is going to matter because it affects how we think about what happen when interests change. It is odd to think that a change in interests could make one know something. But it isn’t as odd to think that a change in interests could block or defeat something that was potentially going to block or defeat an otherwise well supported belief from being knowledge. This is something I will return to repeatedly in <a href="changes.html">chapter&nbsp;<span>7</span></a>.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-AndersonHawthorne2019a" class="csl-entry" role="doc-biblioentry">
Anderson, Charity, and John Hawthorne. 2019a. <span>“Knowledge, Practical Adequacy, and Stakes.”</span> <em>Oxford Studies in Epistemology</em> 6: 234–57.
</div>
<div id="ref-AndersonHawthorne2019b" class="csl-entry" role="doc-biblioentry">
———. 2019b. <span>“Pragmatic Encroachment and Closure.”</span> In <em>Pragmatic Encroachment in Epistemology</em>, edited by Brian Kim and Matthew McGrath, 107–15. New York: Routledge.
</div>
<div id="ref-ArmourGarb2011" class="csl-entry" role="doc-biblioentry">
Armour-Garb, B. 2011. <span>“Contextualism Without Pragmatic Encroachment.”</span> <em>Analysis</em> 71 (4): 667–76. <a href="https://doi.org/10.1093/analys/anr083">https://doi.org/10.1093/analys/anr083</a>.
</div>
<div id="ref-Brown2008" class="csl-entry" role="doc-biblioentry">
Brown, Jessica. 2008. <span>“Subject-Sensitive Invariantism and the Knowledge Norm for Practical Reasoning.”</span> <em>No<span>û</span>s</em> 42 (2): 167–89. <a href="https://doi.org/10.1111/j.1468-0068.2008.00677.x">https://doi.org/10.1111/j.1468-0068.2008.00677.x</a>.
</div>
<div id="ref-Cohen2004" class="csl-entry" role="doc-biblioentry">
Cohen, Stewart. 2004. <span>“Knowledge, Assertion, and Practical Reasoning.”</span> <em>Philosophical Issues</em> 14 (1): 482–91. <a href="https://doi.org/10.1111/j.1533-6077.2004.00040.x">https://doi.org/10.1111/j.1533-6077.2004.00040.x</a>.
</div>
<div id="ref-DeRose2002" class="csl-entry" role="doc-biblioentry">
DeRose, Keith. 2002. <span>“Assertion, Knowledge and Context.”</span> <em>Philosophical Review</em> 111 (2): 167–203. <a href="https://doi.org/10.2307/3182618">https://doi.org/10.2307/3182618</a>.
</div>
<div id="ref-FantlMcGrath2002" class="csl-entry" role="doc-biblioentry">
Fantl, Jeremy, and Matthew McGrath. 2002. <span>“Evidence, Pragmatics, and Justification.”</span> <em>Philosophical Review</em> 111 (1): 67–94. <a href="https://doi.org/10.2307/3182570">https://doi.org/10.2307/3182570</a>.
</div>
<div id="ref-FantlMcGrath2009" class="csl-entry" role="doc-biblioentry">
———. 2009. <em>Knowledge in an Uncertain World</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Ganson2019" class="csl-entry" role="doc-biblioentry">
Ganson, Dorit. 2019. <span>“Great Expectations: Belief and the Case for Pragmatic Encroachment.”</span> In <em>Pragmatic Encroachment in Epistemology</em>, edited by Brian Kim and Matthew McGrath. New York: Routledge.
</div>
<div id="ref-Hawthorne2004" class="csl-entry" role="doc-biblioentry">
Hawthorne, John. 2004. <em>Knowledge and Lotteries</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Lasonen-Aarnio2010b" class="csl-entry" role="doc-biblioentry">
Lasonen-Aarnio, Maria. 2010. <span>“Unreasonable Knowledge.”</span> <em>Philosophical Perspectives</em> 24: 1–21. <a href="https://doi.org/10.1111/j.1520-8583.2010.00183.x">https://doi.org/10.1111/j.1520-8583.2010.00183.x</a>.
</div>
<div id="ref-Lasonen-Aarnio2014" class="csl-entry" role="doc-biblioentry">
———. 2014. <span>“Higher-Order Evidence and the Limits of Defeat.”</span> <em>Philosophy and Phenomenological Research</em> 88 (2): 314–45. <a href="https://doi.org/10.1111/phpr.12090">https://doi.org/10.1111/phpr.12090</a>.
</div>
<div id="ref-MacFarlane2005-Knowledge" class="csl-entry" role="doc-biblioentry">
MacFarlane, John. 2005. <span>“The Assessment Sensitivity of Knowledge Attributions.”</span> <em>Oxford Studies in Epistemology</em> 1: 197–233.
</div>
<div id="ref-MaitraWeatherson2010" class="csl-entry" role="doc-biblioentry">
Maitra, Ishani, and Brian Weatherson. 2010. <span>“Assertion, Knowledge and Action.”</span> <em>Philosophical Studies</em> 149 (1): 99–118. <a href="https://doi.org/10.1007/s11098-010-9542-z">https://doi.org/10.1007/s11098-010-9542-z</a>.
</div>
<div id="ref-Nagel2014" class="csl-entry" role="doc-biblioentry">
Nagel, Jennifer. 2014. <em>Knowledge: A Very Short Introduction</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Nozick1981" class="csl-entry" role="doc-biblioentry">
Nozick, Robert. 1981. <em>Philosophical Explorations</em>. Cambridge, MA: Harvard University Press.
</div>
<div id="ref-Stanley2005" class="csl-entry" role="doc-biblioentry">
Stanley, Jason. 2005. <em><span class="nocase">Knowledge and Practical Interests</span></em>. Oxford University Press.
</div>
<div id="ref-Unger1975" class="csl-entry" role="doc-biblioentry">
Unger, Peter. 1975. <em>Ignorance: A Case for Scepticism</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Weatherson2016" class="csl-entry" role="doc-biblioentry">
Weatherson, Brian. 2016. <span>“Games, Beliefs and Credences.”</span> <em>Philosophy and Phenomenological Research</em> 92 (2): 209–36. <a href="https://doi.org/10.1111/phpr.12088">https://doi.org/10.1111/phpr.12088</a>.
</div>
<div id="ref-Weatherson2017-WEAII" class="csl-entry" role="doc-biblioentry">
———. 2017. <span>“Interest-Relative Invariantism.”</span> In <em>The Routledge Handbook of Epistemic Contextualism</em>, edited by Jonathan Ichikawa, 240–53. Routledge.
</div>
<div id="ref-Weatherson2018-WEAIEA-2" class="csl-entry" role="doc-biblioentry">
———. 2018. <span>“Interests, Evidence and Games.”</span> <em>Episteme</em> 15 (3): 329–44.
</div>
<div id="ref-Weatherson2019" class="csl-entry" role="doc-biblioentry">
———. 2019. <em>Normative Externalism</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Williamson2000" class="csl-entry" role="doc-biblioentry">
Williamson, Timothy. 2000. <em><span class="nocase">Knowledge and its Limits</span></em>. Oxford University Press.
</div>
<div id="ref-Williamson2005" class="csl-entry" role="doc-biblioentry">
———. 2005. <span>“<span class="nocase">Contextualism, Subject-Sensitive Invariantism and Knowledge of Knowledge</span>.”</span> <em>The Philosophical Quarterly</em> 55 (219): 213–35. <a href="https://doi.org/10.1111/j.0031-8094.2005.00396.x">https://doi.org/10.1111/j.0031-8094.2005.00396.x</a>.
</div>
</div>
</section>


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./background.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Background</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./beliefs.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Belief</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>